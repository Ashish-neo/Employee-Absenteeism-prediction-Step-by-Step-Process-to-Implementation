# -*- coding: utf-8 -*-
"""Absenteeism_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-qMez7gigMBdgY3CPEwgeYOUZb_S2_5r
"""

import pandas as pd
import numpy as np

raw_data=pd.read_csv("/content/Absenteeism_dataset.csv")

df=raw_data.copy()

pd.options.display.max_columns=None
pd.options.display.max_rows=None

display(df)

df.info()

df=df.drop(['ID'],axis=1)



df['Reason for Absence'].min()

df['Reason for Absence'].max()

pd.unique(df['Reason for Absence'])

reson_cols=pd.get_dummies(df['Reason for Absence'])

reson_cols['check']=reson_cols.sum(axis=1)
reson_cols

reson_cols['check'].sum(axis=0)

reson_cols['check'].unique()

reson_cols=reson_cols.drop(['check'],axis=1)
reson_cols

reson_cols=pd.get_dummies(df['Reason for Absence'],drop_first=True)

reson_cols

reason_1=reson_cols.iloc[:,1:14].max(axis=1)
reason_2=reson_cols.iloc[:,15:17].max(axis=1)
reason_3=reson_cols.iloc[:,18:21].max(axis=1)
reason_4=reson_cols.iloc[:,22:].max(axis=1)

df=pd.concat([df,reason_1,reason_2,reason_3,reason_4],axis=1)
df

df.columns.values

columns_names=['Reason for Absence', 'Date', 'Transportation Expense',
       'Distance to Work', 'Age', 'Daily Work Load Average',
       'Body Mass Index', 'Education', 'Children', 'Pets',
       'Absenteeism Time in Hours', 'reason_1', 'reason_2', 'reason_3', 'reason_4']

df.columns=columns_names

df.head()

df_copy=df.copy()
df_copy.head()

type(df_copy['Date'])

df_copy['Date']=pd.to_datetime(df_copy['Date'],format='%d/%m/%Y')

df_copy['Date']

df_copy.info()

"""# Now we apply Machine leaning algorithn on our preprocessed data"""

data_pre=pd.read_csv('/content/Absenteeism_preprocessing_dataset.csv')

data_pre.head()

#create targets
data_pre['Absenteeism Time in Hours'].median()

targets=np.where(data_pre['Absenteeism Time in Hours'] >
                 data_pre['Absenteeism Time in Hours'].median(),1,0)

data_pre['Excessive Absenteeism']=targets
data_pre.head()

#we are adding break points
data_process=data_pre.copy()
data_process.head()

targets.sum()/targets.shape[0]

data_with_target=data_process.drop(['Absenteeism Time in Hours'],axis=1)

data_with_target is data_process

data_with_target.head()

#select the inputs for regression
data_with_target.shape

unscale_inputs=data_with_target.iloc[:,0:14] #we use also these iloc[:,:14] or iloc[:,:-1]

#standarized the data
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()

scaler.fit(unscale_inputs)

scaled_inputs=scaler.transform(unscale_inputs)

scaled_inputs.shape

#split the dataset into train and test
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(scaled_inputs,targets,train_size=0.8)

print(x_train.shape,y_train.shape)

print(x_test.shape,y_test.shape)

#load linear model
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

#train the model
reg=LogisticRegression()
reg.fit(x_train,y_train)

reg.score(x_train,y_train)

#check the accuracy
out=reg.predict(x_train)
out

np.sum(out == y_train)

out.shape[0]

np.sum(out == y_train)/out.shape[0]

#finding intercept and coefficients
reg.intercept_

reg.coef_

features_name=unscale_inputs.columns.values

summary_table=pd.DataFrame(columns=['features name'],data=features_name)
summary_table['Coefficients']=np.transpose(reg.coef_)
summary_table

summary_table.index=summary_table.index + 1
summary_table.loc[0]=['intercept',reg.intercept_[0]]
summary_table=summary_table.sort_index()
summary_table

#interpreting the coefficient
summary_table['Odds_ratio']=np.exp(summary_table.Coefficients)
summary_table.sort_values(['Odds_ratio'],ascending=False)
summary_table

#test the model
reg.score(x_test,y_test)

predicted=reg.predict_proba(x_test)
predicted

predicted.shape

predicted[:,1]

#save the model
import pickle

with open('model','wb') as file:
  pickle.dump(reg,file)

